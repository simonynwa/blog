<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习笔记（2） | simonynwa's blog</title><meta name="author" content="Simon Ding"><meta name="copyright" content="Simon Ding"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一. K-means1. 简述k-means流程（1）适当选择c个类的初始中心；（2）在第k次迭代中，对任意一个样本，求其到c各中心的距离，将该样本归到距离最短的那个中心所在的类；（3）利用均值等方法更新该类的中心值；（4）对于所有的C个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束；否则继续迭代。 2. K— means对异常值是否敏感，为什么？K-means对异常值较为敏">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记（2）">
<meta property="og:url" content="http://example.com/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/index.html">
<meta property="og:site_name" content="simonynwa&#39;s blog">
<meta property="og:description" content="一. K-means1. 简述k-means流程（1）适当选择c个类的初始中心；（2）在第k次迭代中，对任意一个样本，求其到c各中心的距离，将该样本归到距离最短的那个中心所在的类；（3）利用均值等方法更新该类的中心值；（4）对于所有的C个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束；否则继续迭代。 2. K— means对异常值是否敏感，为什么？K-means对异常值较为敏">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/avartar.jpg">
<meta property="article:published_time" content="2024-10-10T15:10:40.000Z">
<meta property="article:modified_time" content="2024-10-10T15:11:26.453Z">
<meta property="article:author" content="Simon Ding">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/avartar.jpg"><link rel="shortcut icon" href="/img/web.jpg"><link rel="canonical" href="http://example.com/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习笔记（2）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-10 23:11:26'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/avartar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="simonynwa's blog"><span class="site-name">simonynwa's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习笔记（2）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-10T15:10:40.000Z" title="发表于 2024-10-10 23:10:40">2024-10-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-10T15:11:26.453Z" title="更新于 2024-10-10 23:11:26">2024-10-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习笔记（2）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="一-K-means"><a href="#一-K-means" class="headerlink" title="一. K-means"></a>一. K-means</h3><h4 id="1-简述k-means流程"><a href="#1-简述k-means流程" class="headerlink" title="1. 简述k-means流程"></a>1. 简述k-means流程</h4><p>（1）适当选择c个类的初始中心；<br>（2）在第k次迭代中，对任意一个样本，求其到c各中心的距离，将该样本归到距离最短的那个中心所在的类；<br>（3）利用均值等方法更新该类的中心值；<br>（4）对于所有的C个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束；否则继续迭代。</p>
<h4 id="2-K—-means对异常值是否敏感，为什么？"><a href="#2-K—-means对异常值是否敏感，为什么？" class="headerlink" title="2. K— means对异常值是否敏感，为什么？"></a>2. K— means对异常值是否敏感，为什么？</h4><p>K-means对异常值<strong>较为敏感</strong>，因为一个集合内的元素<strong>均值易受到一个极大值的影响</strong>。当存在异常值的情况下，均值所计算出来的中心位置很可能不能够反映真实的类别中心。</p>
<h4 id="3-如何评估聚类效果？"><a href="#3-如何评估聚类效果？" class="headerlink" title="3. 如何评估聚类效果？"></a>3. 如何评估聚类效果？</h4><p>紧凑度(类内)和分离度(类间)。聚类质量：因为是无监督学习，所以一般通过评估类的分离情况来决定聚类质量。类内越紧密，类间距离越大则质量越高。</p>
<h4 id="4-超参数k如何选择？"><a href="#4-超参数k如何选择？" class="headerlink" title="4. 超参数k如何选择？"></a>4. 超参数k如何选择？</h4><p>手肘法的核心指标是SSE(sum of the squared errors，误差平方和)<br>$$\large SSE &#x3D; \sum_{i&#x3D;1}^{k}\sum_{p \in C_i}|p-m_i|^2$$<br>其中，Ci是第i个簇，p是Ci中的样本点，mi是Ci的质心（Ci中所有样本的均值），SSE是所有样本的聚类误差，代表了聚类效果的好坏。</p>
<p> 手肘法的核心思想是：随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。当然，这也是该方法被称为手肘法的原因。</p>
<h4 id="5-K-means的优缺点"><a href="#5-K-means的优缺点" class="headerlink" title="5. K-means的优缺点"></a>5. K-means的优缺点</h4><p>优点：</p>
<p>算法简单，容易理解，在某些情况下聚类效果不错；</p>
<p>处理大数据集的时候，该算法可以保证较好的伸缩性——即稍作改进即可以处理大数据集；</p>
<p>当数据簇近似于高斯分布时，效果非常不错；</p>
<p>算法复杂度低——时间复杂度为O（nkt），其中n为样本数，k为类别数，t为迭代数。</p>
<p>缺点：</p>
<p>k值需要人为设定，不同的k值得到的结果不一样；</p>
<p>对初始的类别中心敏感，不同选取方法得到的结果不一样；</p>
<p>对异常值敏感；</p>
<p>一个样本只能归为一类，不适合做单样本多标签分类任务；</p>
<h3 id="二-KNN"><a href="#二-KNN" class="headerlink" title="二. KNN"></a>二. KNN</h3><h4 id="1-简述KNN"><a href="#1-简述KNN" class="headerlink" title="1. 简述KNN"></a>1. 简述KNN</h4><ol>
<li><p>计算已知类别数据集中的点与当前点之间的距离；</p>
</li>
<li><p>按照距离递增次序排序；</p>
</li>
<li><p>选取与当前点距离最小的k个点；</p>
</li>
<li><p>确定前k个点所在类别的出现频率；</p>
</li>
<li><p>返回前k个点所出现频率最高的类别作为当前点的预测分类。</p>
</li>
</ol>
<h4 id="2-KNN和K-means的相似和区别"><a href="#2-KNN和K-means的相似和区别" class="headerlink" title="2. KNN和K-means的相似和区别"></a>2. KNN和K-means的相似和区别</h4><p>![[Pasted image 20240821140259.png]]</p>
<h3 id="三-SVM"><a href="#三-SVM" class="headerlink" title="三. SVM"></a>三. SVM</h3><h4 id="1-简述SVM原理"><a href="#1-简述SVM原理" class="headerlink" title="1. 简述SVM原理"></a>1. 简述SVM原理</h4><p>Support Vector Machine 支持向量机，主要用于解决分类问题，属于有监督学习算法的一种。</p>
<p>![[Pasted image 20240823101901.png]]</p>
<p>在保证决策面方向不变且不会出现错分样本的情况下移动决策面，会在<strong>原来的决策面两侧找到两个极限位置（越过该位置就会产生错分现象）</strong>，如虚线所示。虚线的位置由决策面的方向和距离原决策面最近的几个样本的位置决定。而这两条平行虚线正中间的分界线就是在保持当前决策面方向不变的前提下的最优决策面。<strong>两条虚线之间的垂直距离就是这个最优决策面对应的分类间隔</strong> 显然每一个可能把数据集正确分开的方向都有一个最优决策面（有些方向无论如何移动决策面的位置也不可能将两类样本完全分开），而不同方向的最优决策面的分类间隔通常是不同的，那个具有“最大间隔”的决策面就是SVM要寻找的最优解。而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为“支持向量”。对于图1中的数据，A决策面就是SVM寻找的最优解，而相应的三个位于虚线上的样本点在坐标系中对应的向量就叫做支持向量。</p>
<p>SVM是一种二分类模型，它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。</p>
<ul>
<li>当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。</li>
<li>当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机。</li>
<li>当训练数据线性不可分时，通过使用核技巧和软间隔最大化，学习非线性支持向量机。</li>
</ul>
<h4 id="2-SVM大致过程"><a href="#2-SVM大致过程" class="headerlink" title="2. SVM大致过程"></a>2. SVM大致过程</h4><p>设立两个超平面函数，并用函数表示他们之间的距离关系。</p>
<p>将求解支持平面最大距离的问题转化为凸优化问题，因为我们有了我们的要求解的距离函数，但是此时该函数需要服从一定的情况（就是两个面不能越过支撑点，该约束条件明显是一个不等式，所以用KKT条件来做优化，如果是等式约束就用拉格朗日算子来做优化）。</p>
<p>对式子求梯度，并且当梯度为0时，该式可以取得最大值。</p>
<h4 id="3-SVM为什么采用间隔最大化"><a href="#3-SVM为什么采用间隔最大化" class="headerlink" title="3. SVM为什么采用间隔最大化"></a>3. SVM为什么采用间隔最大化</h4><p>当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过次时代解有无穷多个。线性可分支持向量机利用间隔最大化求得最有分离超平面，这时解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的。对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。</p>
<h4 id="4-SVM为什么引入核函数"><a href="#4-SVM为什么引入核函数" class="headerlink" title="4. SVM为什么引入核函数"></a>4. SVM为什么引入核函数</h4><p>当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。而引入这样的映射后，无需求解真实的映射函数，只需要知道其核函数：<br>$$\large K(x,y) &#x3D; &lt;\phi(x), \phi(y)&gt;$$<br>即在特征空间内的内积等于它们在原始样本空间中通过核函数K计算的结果。</p>
<h4 id="5-SVM核函数之间的区别"><a href="#5-SVM核函数之间的区别" class="headerlink" title="5. SVM核函数之间的区别"></a>5. SVM核函数之间的区别</h4><p>一般选择线性核和高斯核（RBF核）。</p>
<p>线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。</p>
<p>RBF核：主要用于线性不可分的情形，参数多，分裂结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。</p>
<p>如果Feature的数量很大，跟样本数量差不多，选择线性核的SVM，反之选择RBF核。</p>
<h4 id="6-SVM优点和缺点"><a href="#6-SVM优点和缺点" class="headerlink" title="6.SVM优点和缺点"></a>6.SVM优点和缺点</h4><p>优点：</p>
<ol>
<li>它基于结构风险最小化原则，这样就避免了过学习问题，泛化能力强。</li>
<li>它是一个凸优化问题，因此局部最优解一定是全局最优解。</li>
</ol>
<p>缺点：</p>
<ol>
<li>对缺失数据敏感</li>
<li>对参数和核函数的选择敏感</li>
</ol>
<h3 id="四-决策树"><a href="#四-决策树" class="headerlink" title="四. 决策树"></a>四. 决策树</h3><h4 id="1-简述决策树的构建过程"><a href="#1-简述决策树的构建过程" class="headerlink" title="1. 简述决策树的构建过程"></a>1. 简述决策树的构建过程</h4><p>步骤1：将所有的特征看成一个一个的节点，eg（拥有房产，婚姻状态，年收入）</p>
<p>步骤2：将特征按不同属性分割（比如年龄这个特征：可以按老中青三个属性来划分），然后求出不同特征的信息增益&#x2F;信息增益率&#x2F;gini系数等，不同的树取不同的值。</p>
<p>步骤3：使用第二步遍历所有特征，选择出最优的特征，以及该特征的最优的划分方式，得出最终的子节点N1、 N2….Nm。</p>
<p>步骤4：对子节点N1、N2….Nm分别继续执行2-3步，直到每个最终的子节点都足够“纯”。</p>
<p>如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了。</p>
<h4 id="2-ID3决策树与C4-5决策树的区别"><a href="#2-ID3决策树与C4-5决策树的区别" class="headerlink" title="2.  ID3决策树与C4.5决策树的区别"></a>2.  ID3决策树与C4.5决策树的区别</h4><p>ID3和5算法均只适合在小规模数据集上使用。</p>
<p>ID3和5算法都是单变量决策树。当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差。 决策树分类一般情况只适合小数据量的情况(数据可以放内存) CART算法是三种算法中最常用的。</p>
<h4 id="3-ID3算法和CART算法"><a href="#3-ID3算法和CART算法" class="headerlink" title="3. ID3算法和CART算法"></a>3. ID3算法和CART算法</h4><p>ID3算法：</p>
<ol>
<li>对当前例子集合，计算属性的信息增益</li>
<li>选择信息增益大的属性A</li>
<li>把在A处取值相同的例子归于同一子集，A取几个值就得几个子集</li>
<li>一次对每种取值情况下的子集，返回1递归调用建树算法</li>
<li>若子集中只含有单个属性，则分值为叶子节点，判断其属性值并标上相应的符号，然后返回调用处。</li>
</ol>
<p>CART<br>CART分类回归树是一种典型的二叉决策树，可以做分类或者回归。如果待预测结果是离散型数据，则CART生成分类决策树；如果待预测结果是连续型数据，则CART生成回归决策树。</p>
<p>C4.5</p>
<p>C4.5是ID3的继承者，相对于ID3算法，C4.5算法的改进主要有：</p>
<p>●增加了对连续特征属性的处理，通过排序连续属性值并挑选阈值，将连续特征属性值划分为高于阈值的属性和小于或等于阈值的属性。</p>
<p>●增加了对属性值缺失的训练数据的处理。</p>
<p>●挑选特征属性依据信息增益率，而不是信息增益。</p>
<p>●创建树后进行修剪，试图通过用叶子节点进行替换来删除那些没有帮助的分支。</p>
<h4 id="4-决策树的优缺点："><a href="#4-决策树的优缺点：" class="headerlink" title="4. 决策树的优缺点："></a>4. 决策树的优缺点：</h4><p>优点:</p>
<p>(1)速度快: 计算量相对较小, 且容易转化成分类规则. 只要沿着树根向下一直走到叶, 沿途的分裂条件就能够唯一确定一条分类的谓词.</p>
<p>(2)准确性高: 挖掘出来的分类规则准确性高, 便于理解, 决策树可以清晰的显示哪些字段比较重要, 即可以生成可以理解的规则.</p>
<p>(3)可以处理连续和种类字段</p>
<p>(4)不需要任何领域知识和参数假设</p>
<p>(5)适合高维数据</p>
<p>缺点:</p>
<p>(1)对于各类别样本数量不一致的数据, 信息增益偏向于那些更多数值的特征</p>
<p>(2)容易过拟合</p>
<p>(3)忽略属性之间的相关性。</p>
<h3 id="五-神经网络基础"><a href="#五-神经网络基础" class="headerlink" title="五.  神经网络基础"></a>五.  神经网络基础</h3><h4 id="1-为什么必须在神经网络中引入非线性"><a href="#1-为什么必须在神经网络中引入非线性" class="headerlink" title="1. 为什么必须在神经网络中引入非线性"></a>1. 为什么必须在神经网络中引入非线性</h4><p>如果不用激励函数（其实相当于激励函数是f(x) &#x3D; x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当。</p>
<h4 id="2-Relu的优缺点"><a href="#2-Relu的优缺点" class="headerlink" title="2. Relu的优缺点"></a>2. Relu的优缺点</h4><p>有的负值都变为0，而正值不变，这种操作被成为单侧抑制。正因为有了这单侧抑制，才使得神经网络中的神经元也具有了<strong>稀疏激活性</strong>。</p>
<p>使用 ReLU 得到的 SGD 的收敛速度会比 sigmoid&#x2F;tanh 快很多; 计算复杂度低，不需要进行指数运算；</p>
<p>训练的时候很”脆弱”，很容易就”die”了; ReLU不会对数据做幅度压缩，所以数据的幅度会随着模型层数的增加不断扩张</p>
<h4 id="3-四种归一化"><a href="#3-四种归一化" class="headerlink" title="3. 四种归一化"></a>3. 四种归一化</h4><p>为什么要归一化：神经网络学习过程的本质就是为了学习数据分布，如果我们没有做归一化处理，那么每一批次训练数据的分布不一样，从大的方向上看，神经网络则需要在这多个分布中找到平衡点，从小的方向上看，由于每层网络输入数据分布在不断变化，这也会导致每层网络在找平衡点，显然，神经网络就很难收敛了。当然，如果我们只是对输入的数据进行归一化处理（比如将输入的图像除以255，将其归到0到1之间），只能保证输入层数据分布是一样的，并不能保证每层网络输入数据分布是一样的，所以也需要在神经网络的中间层加入归一化处理。神经网络学习过程本质上就是为了学习数据分布，如果训练数据与测试数据的分布不同，网络的泛化能力就会严重降低。（<strong>任何层都需要归一化</strong>）</p>
<h4 id="4-什么是端到端学习（end-to-end）"><a href="#4-什么是端到端学习（end-to-end）" class="headerlink" title="4. 什么是端到端学习（end-to-end）"></a>4. 什么是端到端学习（end-to-end）</h4><p>端到端学习是一种解决问题的思路，与之对应的是多步骤解决问题，也就是将一个问题拆分为多个步骤分步解决，而端到端是由输入端的数据直接得到输出端的结果。</p>
<h4 id="5-AlexNet，VGG，GoogleNet，ResNet等网络之间的区别是什么"><a href="#5-AlexNet，VGG，GoogleNet，ResNet等网络之间的区别是什么" class="headerlink" title="5. AlexNet，VGG，GoogleNet，ResNet等网络之间的区别是什么?"></a>5. AlexNet，VGG，GoogleNet，ResNet等网络之间的区别是什么?</h4><p>Alexnet：</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Simon Ding</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/">http://example.com/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">simonynwa's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/avartar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/" title="机器学习笔记（1）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">机器学习笔记（1）</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/avartar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Simon Ding</div><div class="author-info__description">我的个人博客</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/simonynwa"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/simonynwa" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dingyuang@sjtu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80-K-means"><span class="toc-number">1.</span> <span class="toc-text">一. K-means</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%AE%80%E8%BF%B0k-means%E6%B5%81%E7%A8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1. 简述k-means流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-K%E2%80%94-means%E5%AF%B9%E5%BC%82%E5%B8%B8%E5%80%BC%E6%98%AF%E5%90%A6%E6%95%8F%E6%84%9F%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">2. K— means对异常值是否敏感，为什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0%E8%81%9A%E7%B1%BB%E6%95%88%E6%9E%9C%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">3. 如何评估聚类效果？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E8%B6%85%E5%8F%82%E6%95%B0k%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">4. 超参数k如何选择？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-K-means%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.5.</span> <span class="toc-text">5. K-means的优缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C-KNN"><span class="toc-number">2.</span> <span class="toc-text">二. KNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%AE%80%E8%BF%B0KNN"><span class="toc-number">2.1.</span> <span class="toc-text">1. 简述KNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-KNN%E5%92%8CK-means%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%92%8C%E5%8C%BA%E5%88%AB"><span class="toc-number">2.2.</span> <span class="toc-text">2. KNN和K-means的相似和区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89-SVM"><span class="toc-number">3.</span> <span class="toc-text">三. SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%AE%80%E8%BF%B0SVM%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">1. 简述SVM原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-SVM%E5%A4%A7%E8%87%B4%E8%BF%87%E7%A8%8B"><span class="toc-number">3.2.</span> <span class="toc-text">2. SVM大致过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-SVM%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%87%E7%94%A8%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text">3. SVM为什么采用间隔最大化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-SVM%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BC%95%E5%85%A5%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">3.4.</span> <span class="toc-text">4. SVM为什么引入核函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-SVM%E6%A0%B8%E5%87%BD%E6%95%B0%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">3.5.</span> <span class="toc-text">5. SVM核函数之间的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-SVM%E4%BC%98%E7%82%B9%E5%92%8C%E7%BC%BA%E7%82%B9"><span class="toc-number">3.6.</span> <span class="toc-text">6.SVM优点和缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">4.</span> <span class="toc-text">四. 决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%AE%80%E8%BF%B0%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">1. 简述决策树的构建过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-ID3%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8EC4-5%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">4.2.</span> <span class="toc-text">2.  ID3决策树与C4.5决策树的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-ID3%E7%AE%97%E6%B3%95%E5%92%8CCART%E7%AE%97%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">3. ID3算法和CART算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="toc-number">4.4.</span> <span class="toc-text">4. 决策树的优缺点：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-number">5.</span> <span class="toc-text">五.  神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%85%E9%A1%BB%E5%9C%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%BC%95%E5%85%A5%E9%9D%9E%E7%BA%BF%E6%80%A7"><span class="toc-number">5.1.</span> <span class="toc-text">1. 为什么必须在神经网络中引入非线性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Relu%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">5.2.</span> <span class="toc-text">2. Relu的优缺点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%9B%9B%E7%A7%8D%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">5.3.</span> <span class="toc-text">3. 四种归一化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%BB%80%E4%B9%88%E6%98%AF%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0%EF%BC%88end-to-end%EF%BC%89"><span class="toc-number">5.4.</span> <span class="toc-text">4. 什么是端到端学习（end-to-end）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-AlexNet%EF%BC%8CVGG%EF%BC%8CGoogleNet%EF%BC%8CResNet%E7%AD%89%E7%BD%91%E7%BB%9C%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">5.5.</span> <span class="toc-text">5. AlexNet，VGG，GoogleNet，ResNet等网络之间的区别是什么?</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/" title="机器学习笔记（2）">机器学习笔记（2）</a><time datetime="2024-10-10T15:10:40.000Z" title="发表于 2024-10-10 23:10:40">2024-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/" title="机器学习笔记（1）">机器学习笔记（1）</a><time datetime="2024-10-10T15:10:23.000Z" title="发表于 2024-10-10 23:10:23">2024-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/python%E8%80%83%E7%82%B9/" title="python考点">python考点</a><time datetime="2024-10-10T15:02:56.000Z" title="发表于 2024-10-10 23:02:56">2024-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/19/hello-world/" title="Hello World">Hello World</a><time datetime="2024-09-19T03:01:22.255Z" title="发表于 2024-09-19 11:01:22">2024-09-19</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By Simon Ding</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">希望自己的日积月累，终能成他人的望尘莫及。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>